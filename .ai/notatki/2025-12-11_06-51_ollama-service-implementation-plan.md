# Sesja: Plan WdroÅ¼enia Ollama Service (Local LLM)

**Data:** 2025-12-11
**Czas:** 06:51
**Asystent:** Auto (Cursor AI)

---

## ğŸ“‹ Cel sesji

Stworzenie kompleksowego planu wdroÅ¼enia usÅ‚ugi Ollama Service dla lokalnego interfejsu API Ollama. UsÅ‚uga ma zapewniÄ‡ peÅ‚nÄ… integracjÄ™ z lokalnym serwisem LLM, w tym obsÅ‚ugÄ™ structured outputs (JSON), zarzÄ…dzanie poÅ‚Ä…czeniami, health checks oraz zaawansowanÄ… obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w specyficznÄ… dla Å›rodowiska lokalnego.

---

## ğŸ¯ Wykonane zadania

### 1. Analiza istniejÄ…cego kodu

#### PrzeglÄ…d istniejÄ…cych implementacji
- **`backend/services/ollama_service.py`**: Podstawowa implementacja z `OLLAMAClient`, health checks i generowaniem embeddingÃ³w
- **`backend/services/llm_service.py`**: Implementacja generowania tekstu z podstawowÄ… obsÅ‚ugÄ… bÅ‚Ä™dÃ³w
- **`backend/config.py`**: Konfiguracja z ustawieniami OLLAMA (host, modele, timeouty)
- **`backend/services/exceptions.py`**: Hierarchia wyjÄ…tkÃ³w dla obsÅ‚ugi bÅ‚Ä™dÃ³w

#### Zidentyfikowane braki
- Brak obsÅ‚ugi structured outputs (JSON format)
- Ograniczona obsÅ‚uga bÅ‚Ä™dÃ³w dla Å›rodowiska lokalnego (cold start, OOM, model not found)
- Brak retry logic z exponential backoff
- Brak zarzÄ…dzania connection pooling
- Brak walidacji modeli przed uÅ¼yciem
- Brak cache'owania health checks i listy modeli

### 2. Stworzenie kompleksowego planu wdroÅ¼enia

#### Struktura dokumentu
Utworzono dokument `.ai/ollama-service-implementation-plan.md` zawierajÄ…cy:

**1. Opis usÅ‚ugi (Local Inference Service)**
- Architektura systemu z diagramem
- Cel i zakres usÅ‚ugi
- Integracja z FastAPI backend

**2. Opis konstruktora (Inicjalizacja klienta Ollama/URL bazowego)**
- Klasa `OllamaService` jako singleton
- Parametry inicjalizacji (base_url, timeouty, retry logic)
- Pola publiczne i prywatne
- PrzykÅ‚ad uÅ¼ycia singleton pattern

**3. Publiczne metody i pola**
- `health_check()` - sprawdzanie dostÄ™pnoÅ›ci z cache'owaniem
- `validate_model()` - walidacja dostÄ™pnoÅ›ci modelu
- `generate_text()` - podstawowe generowanie tekstu
- `generate_text_structured()` - generowanie JSON z walidacjÄ… schematu
- `generate_embedding()` - generowanie wektorÃ³w embedding
- `list_models()` - lista dostÄ™pnych modeli

**4. Prywatne metody i pola**
- `_get_client()` - lazy initialization klienta HTTP
- `_retry_request()` - retry logic z exponential backoff
- `_validate_generation_params()` - walidacja parametrÃ³w generowania
- `_build_structured_system_prompt()` - budowanie promptu z JSON schema
- `_parse_json_response()` - parsowanie i walidacja JSON

**5. ObsÅ‚uga bÅ‚Ä™dÃ³w (Specyficzna dla Self-Hosted)**
SzczegÃ³Å‚owe scenariusze bÅ‚Ä™dÃ³w i rozwiÄ…zania:
- **Ollama nie jest uruchomiona**: Connection refused, health check failure
- **Model nie zostaÅ‚ pobrany**: HTTP 404, komunikat z instrukcjÄ… `ollama pull`
- **Timeout przy generowaniu**: Model-specific timeouts, adaptive timeouts
- **Brak pamiÄ™ci RAM (OOM)**: Wykrywanie i sugestie (mniejszy model, redukcja kontekstu)
- **NieprawidÅ‚owy format JSON**: Parsowanie z fallback, walidacja schematu
- **Cold start modelu**: Warmup mechanism dla szybszego pierwszego uÅ¼ycia

**6. Kwestie bezpieczeÅ„stwa i wydajnoÅ›ci**
- **ZarzÄ…dzanie pamiÄ™ciÄ… RAM**: Lazy loading, monitoring, ograniczenie kontekstu
- **ZarzÄ…dzanie poÅ‚Ä…czeniami HTTP**: Connection pooling, rate limiting
- **Timeout management**: Model-specific timeouts, adaptive timeouts
- **Caching i optymalizacja**: Cache health checks, cache listy modeli
- **BezpieczeÅ„stwo lokalne**: Walidacja Å¼e Ollama nie jest publicznie dostÄ™pna

**7. Plan wdroÅ¼enia krok po kroku**
8 szczegÃ³Å‚owych krokÃ³w:
1. Przygotowanie Å›rodowiska (instalacja Ollama, pobranie modeli)
2. Aktualizacja konfiguracji (.env, config.py)
3. Implementacja OllamaService (wszystkie metody)
4. Integracja z istniejÄ…cym kodem (llm_service.py, rag_pipeline.py)
5. Dodanie structured outputs (helper functions)
6. Testy (unit i integration)
7. Dokumentacja i monitoring
8. Deployment checklist

### 3. Kluczowe funkcjonalnoÅ›ci

#### Structured Outputs (JSON Format)
**Dwa podejÅ›cia:**
1. **Parametr `format: 'json'`** (rekomendowane) - natywne wsparcie Ollama
2. **Schema w System Prompt** (fallback) - wstrzykniÄ™cie schematu JSON do promptu systemowego

**PrzykÅ‚ad uÅ¼ycia:**
```python
schema = {
    "type": "object",
    "properties": {
        "answer": {"type": "string"},
        "sources": {"type": "array", "items": {...}},
        "confidence": {"type": "number"}
    }
}

response = await service.generate_text_structured(
    prompt="Co to jest umowa o pracÄ™?",
    model="mistral:7b",
    json_schema=schema
)
```

#### System Prompt i User Prompt
- **System Prompt**: Definicja roli modelu (ekspert prawny)
- **User Prompt**: Pytanie uÅ¼ytkownika + kontekst prawny
- **Enhanced System Prompt**: Dla structured outputs - system prompt + instrukcje JSON schema

#### Parametry modelu
- **temperature**: 0.3 (niÅ¼sza dla bardziej faktualnych odpowiedzi)
- **top_p**: 0.9 (nucleus sampling)
- **top_k**: 40 (top-k sampling)
- **num_ctx**: Rozmiar okna kontekstu (dostosowany do modelu)
- **seed**: Opcjonalny seed dla reprodukowalnoÅ›ci

#### Nazwy modeli
- **Fast model**: `mistral:7b` (<15s target)
- **Accurate model**: `gpt-oss:120b` (<240s target)
- **Embedding model**: `nomic-embed-text` (768-dim vectors)

### 4. ObsÅ‚uga bÅ‚Ä™dÃ³w - szczegÃ³Å‚y implementacji

#### Health Check z Cache'owaniem
```python
HEALTH_CHECK_CACHE_TTL = 30  # seconds

async def health_check(self, force: bool = False) -> bool:
    if not force:
        elapsed = time.time() - self._last_health_check
        if elapsed < HEALTH_CHECK_CACHE_TTL:
            return self.is_available
    # ... perform check ...
```

#### Retry Logic z Exponential Backoff
```python
wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
await asyncio.sleep(wait_time)
```

#### Walidacja Modelu z Cache'owaniem
- Sprawdzenie cache przed wywoÅ‚aniem API
- Aktualizacja cache po sprawdzeniu
- Komunikat bÅ‚Ä™du z instrukcjÄ… `ollama pull`

### 5. ZarzÄ…dzanie zasobami

#### Connection Pooling
```python
httpx.AsyncClient(
    limits=httpx.Limits(
        max_keepalive_connections=5,
        max_connections=10
    )
)
```

#### Rate Limiting (na poziomie aplikacji)
```python
self._concurrent_requests = Semaphore(3)  # Max 3 concurrent
```

#### Model-Specific Timeouts
```python
MODEL_TIMEOUTS = {
    "mistral:7b": 15,
    "gpt-oss:120b": 240,
    "nomic-embed-text": 30
}
```

---

## ğŸ“ SzczegÃ³Å‚y techniczne

### Architektura
```
FastAPI Backend
    â†“
OllamaService (Singleton)
    â†“ HTTP/REST
Ollama Service (localhost:11434)
    - mistral:7b
    - gpt-oss:120b
    - nomic-embed-text
```

### Deployment Scenarios
Plan uwzglÄ™dnia rÃ³Å¼ne scenariusze wdroÅ¼enia:
- **All-in-one**: Wszystko na localhost
- **Distributed**: Frontend/Backend na jednej maszynie, Ollama na drugiej
- **Cloud**: Wszystkie komponenty w chmurze
- **Hybrid**: Mieszana konfiguracja

### Integration Points
- `backend/services/llm_service.py` - uÅ¼ycie OllamaService zamiast bezpoÅ›rednich wywoÅ‚aÅ„ httpx
- `backend/services/rag_pipeline.py` - integracja z RAG pipeline
- `backend/routers/queries.py` - endpointy API uÅ¼ywajÄ…ce usÅ‚ugi

---

## âœ… Rezultat

Utworzono kompleksowy plan wdroÅ¼enia w pliku:
**`.ai/ollama-service-implementation-plan.md`**

Plan zawiera:
- âœ… SzczegÃ³Å‚owy opis wszystkich komponentÃ³w
- âœ… PrzykÅ‚ady kodu dla kaÅ¼dej funkcjonalnoÅ›ci
- âœ… ObsÅ‚ugÄ™ bÅ‚Ä™dÃ³w dla 6 scenariuszy
- âœ… ZarzÄ…dzanie zasobami (pamiÄ™Ä‡, poÅ‚Ä…czenia, timeouty)
- âœ… 8-krokowy plan wdroÅ¼enia
- âœ… PrzykÅ‚ady uÅ¼ycia i troubleshooting
- âœ… Deployment checklist

Plan jest gotowy do uÅ¼ycia przez developera i zawiera wszystkie niezbÄ™dne informacje do prawidÅ‚owego wdroÅ¼enia usÅ‚ugi.

---

## ğŸ”„ NastÄ™pne kroki

1. **Implementacja OllamaService** zgodnie z planem
2. **Aktualizacja istniejÄ…cego kodu** (llm_service.py, rag_pipeline.py)
3. **Dodanie structured outputs** dla ustrukturyzowanych odpowiedzi
4. **Testy** (unit i integration)
5. **Monitoring i logging** w produkcji

---

## ğŸ“š Zasoby

- **Dokumentacja Ollama API**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **JSON Schema**: https://json-schema.org/
- **httpx Documentation**: https://www.python-httpx.org/

---

**Status:** âœ… UkoÅ„czone
**Plik:** `.ai/ollama-service-implementation-plan.md`

