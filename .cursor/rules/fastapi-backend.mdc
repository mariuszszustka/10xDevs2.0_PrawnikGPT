---
description: Rules for FastAPI backend files in PrawnikGPT
globs: backend/**/*.py
alwaysApply: false
---

# Backend Guidelines (FastAPI + Python)

## Project Structure

```
backend/
├── main.py              # FastAPI app entry point
├── routers/             # API route handlers
│   ├── __init__.py
│   ├── queries.py       # POST /queries, GET /queries
│   ├── responses.py     # POST /queries/{id}/detailed-response
│   ├── ratings.py       # POST /responses/{id}/ratings
│   └── auth.py          # Auth endpoints (optional wrappers)
├── services/            # Business logic
│   ├── __init__.py
│   ├── rag_pipeline.py  # RAG orchestration
│   ├── embedding_service.py  # OLLAMA embeddings
│   ├── llm_service.py   # OLLAMA text generation
│   └── vector_search.py # Supabase pgvector queries
├── models/              # Pydantic models
│   ├── __init__.py
│   ├── query.py         # QueryRequest, QueryResponse
│   ├── response.py      # ResponseCreate, ResponseOut
│   └── rating.py        # RatingCreate, RatingOut
├── db/                  # Database utilities
│   ├── __init__.py
│   ├── supabase_client.py  # Supabase client setup
│   └── queries.py       # Database query functions
├── middleware/          # FastAPI middleware
│   ├── __init__.py
│   ├── auth.py          # JWT validation
│   └── rate_limit.py    # Rate limiting
├── tests/               # Tests (pytest)
│   ├── test_rag_pipeline.py
│   ├── test_endpoints.py
│   └── conftest.py      # Pytest fixtures
├── requirements.txt     # Python dependencies
└── .env                 # Environment variables (not committed)
```

---

## FastAPI Best Practices

### 1. App Structure (main.py)
```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from routers import queries, responses, ratings

app = FastAPI(
    title="PrawnikGPT API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:4321"],  # Astro dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(queries.router, prefix="/queries", tags=["queries"])
app.include_router(responses.router, prefix="/responses", tags=["responses"])
app.include_router(ratings.router, prefix="/ratings", tags=["ratings"])

@app.get("/health")
async def health_check():
    return {"status": "ok"}
```

### 2. Router Structure
```python
# routers/queries.py
from fastapi import APIRouter, Depends, HTTPException, status
from models.query import QueryRequest, QueryResponse
from services.rag_pipeline import generate_fast_response
from middleware.auth import get_current_user

router = APIRouter()

@router.post("", response_model=QueryResponse, status_code=status.HTTP_201_CREATED)
async def create_query(
    request: QueryRequest,
    user_id: str = Depends(get_current_user)
):
    """
    Create a new query and generate fast response (<15s).

    Args:
        request: Query request with question text
        user_id: Authenticated user ID from JWT

    Returns:
        QueryResponse with fast answer

    Raises:
        HTTPException: 400 if question invalid, 404 if no relevant acts found
    """
    # Validation
    if not (10 <= len(request.question) <= 1000):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Question must be 10-1000 characters"
        )

    # Generate response
    try:
        response = await generate_fast_response(request.question, user_id)
        return response
    except NoRelevantActsError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Moja baza wiedzy jest ograniczona do 20k najnowszych ustaw"
        )
```

### 3. Pydantic Models (Request/Response Validation)
```python
# models/query.py
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime

class Source(BaseModel):
    act_title: str
    article: str
    link: str

class ResponseOut(BaseModel):
    id: int
    response_type: str = Field(..., pattern="^(fast|detailed)$")
    content: str
    sources: List[Source]
    model_name: str
    generation_time_ms: int

class QueryRequest(BaseModel):
    question: str = Field(..., min_length=10, max_length=1000)

class QueryResponse(BaseModel):
    query_id: int
    question: str
    response: ResponseOut
    created_at: datetime
```

### 4. Dependency Injection (JWT Auth)
```python
# middleware/auth.py
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import jwt, JWTError

security = HTTPBearer()

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> str:
    """
    Extract and validate JWT token, return user_id.

    Raises:
        HTTPException: 401 if token invalid or expired
    """
    token = credentials.credentials

    try:
        # Verify JWT with Supabase public key
        payload = jwt.decode(
            token,
            SUPABASE_JWT_SECRET,
            algorithms=["HS256"]
        )
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
        return user_id
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials"
        )
```

---

## Error Handling

### 1. Custom Exceptions
```python
# services/exceptions.py
class PrawnikGPTError(Exception):
    """Base exception for PrawnikGPT"""
    pass

class NoRelevantActsError(PrawnikGPTError):
    """Raised when no relevant legal acts found"""
    pass

class OLLAMATimeoutError(PrawnikGPTError):
    """Raised when OLLAMA request times out"""
    pass

class EmbeddingGenerationError(PrawnikGPTError):
    """Raised when embedding generation fails"""
    pass
```

### 2. Error Handler Middleware
```python
from fastapi import Request, status
from fastapi.responses import JSONResponse

@app.exception_handler(PrawnikGPTError)
async def prawnikgpt_exception_handler(request: Request, exc: PrawnikGPTError):
    """Global error handler for custom exceptions"""
    if isinstance(exc, NoRelevantActsError):
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"detail": str(exc)}
        )
    elif isinstance(exc, OLLAMATimeoutError):
        return JSONResponse(
            status_code=status.HTTP_408_REQUEST_TIMEOUT,
            content={"detail": "Request timeout. Please try again."}
        )
    else:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"detail": "Internal server error"}
        )
```

---

## Async/Await (Critical for Performance)

### 1. Always Use Async for I/O Operations
```python
import httpx  # Use httpx instead of requests for async

async def generate_embedding(text: str) -> list[float]:
    """Generate embedding using OLLAMA API (async)"""
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            f"{OLLAMA_HOST}/api/embeddings",
            json={"model": "nomic-embed-text", "prompt": text}
        )
        response.raise_for_status()
        return response.json()["embedding"]
```

### 2. Concurrent Requests with asyncio.gather
```python
import asyncio

async def generate_embeddings_batch(texts: list[str]) -> list[list[float]]:
    """Generate embeddings for multiple texts concurrently"""
    tasks = [generate_embedding(text) for text in texts]
    return await asyncio.gather(*tasks)
```

### 3. Database Queries with Async Supabase
```python
from supabase import create_async_client

async def get_query(query_id: int, user_id: str) -> dict:
    """Fetch query from database (async)"""
    supabase = await create_async_client(SUPABASE_URL, SUPABASE_KEY)

    response = await supabase.table("queries").select("*").eq("id", query_id).eq("user_id", user_id).single().execute()

    if not response.data:
        raise HTTPException(status_code=404, detail="Query not found")

    return response.data
```

---

## RAG Pipeline Service

### services/rag_pipeline.py
```python
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

async def generate_fast_response(question: str, user_id: str) -> Dict:
    """
    Full RAG pipeline for fast response generation.

    Steps:
    1. Generate query embedding
    2. Similarity search in pgvector
    3. Fetch related acts (graph traversal)
    4. Construct prompt
    5. Generate LLM response
    6. Parse sources
    7. Store in database
    8. Cache context (5 min TTL)

    Args:
        question: User question (10-1000 chars)
        user_id: Authenticated user ID

    Returns:
        Dict with query_id, response_id, content, sources, etc.

    Raises:
        NoRelevantActsError: If no chunks found or similarity too low
        OLLAMATimeoutError: If LLM generation times out (>15s)
    """
    import time
    start_time = time.time()

    # Step 1: Embed question
    query_embedding = await generate_embedding(question)
    logger.info(f"Generated embedding in {time.time() - start_time:.2f}s")

    # Step 2: Similarity search
    chunks = await semantic_search(query_embedding, top_k=10)
    if not chunks or chunks[0]["distance"] > 0.5:
        raise NoRelevantActsError("No relevant legal acts found")

    logger.info(f"Found {len(chunks)} relevant chunks")

    # Step 3: Fetch related acts
    act_ids = list(set(c["act_id"] for c in chunks))
    related_acts = await fetch_related_acts(act_ids, max_depth=2)

    # Step 4: Construct prompt
    prompt = construct_prompt(question, chunks, related_acts)

    # Step 5: Generate LLM response (timeout 15s)
    try:
        llm_response = await generate_llm_response(
            prompt,
            model="mistral:7b",
            timeout=15
        )
    except asyncio.TimeoutError:
        raise OLLAMATimeoutError("Fast response generation timed out")

    # Step 6: Parse sources
    sources = extract_sources(llm_response, chunks)

    # Step 7: Store in database
    query_id = await store_query(user_id, question)
    response_data = {
        "query_id": query_id,
        "response_type": "fast",
        "content": llm_response,
        "sources": sources,
        "model_name": "mistral:7b",
        "generation_time_ms": int((time.time() - start_time) * 1000)
    }
    response_id = await store_response(response_data)

    # Step 8: Cache context
    await cache_rag_context(query_id, chunks, related_acts)

    logger.info(f"Fast response generated in {time.time() - start_time:.2f}s")

    return {
        "query_id": query_id,
        "question": question,
        "response": {
            "id": response_id,
            **response_data
        }
    }
```

---

## Database Queries (Supabase)

### db/queries.py
```python
from supabase import create_client, Client
from typing import List, Dict, Optional

supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

async def semantic_search(
    query_embedding: list[float],
    top_k: int = 10
) -> List[Dict]:
    """
    Perform similarity search using pgvector.

    Uses Supabase RPC function for optimal performance.
    """
    response = await supabase.rpc(
        "semantic_search_chunks",
        {
            "query_embedding": query_embedding,
            "match_count": top_k
        }
    ).execute()

    return response.data

async def fetch_related_acts(
    act_ids: List[int],
    max_depth: int = 2
) -> List[Dict]:
    """
    Fetch related acts using recursive CTE (graph traversal).
    """
    response = await supabase.rpc(
        "fetch_related_acts",
        {
            "act_ids": act_ids,
            "max_depth": max_depth
        }
    ).execute()

    return response.data

async def store_query(user_id: str, question: str) -> int:
    """Insert query into database, return query_id"""
    response = await supabase.table("queries").insert({
        "user_id": user_id,
        "question": question
    }).execute()

    return response.data[0]["id"]
```

---

## Testing with Pytest

### tests/test_rag_pipeline.py
```python
import pytest
from unittest.mock import AsyncMock, patch
from services.rag_pipeline import generate_fast_response
from services.exceptions import NoRelevantActsError

@pytest.mark.asyncio
async def test_generate_fast_response_success():
    """Test successful fast response generation"""
    with patch("services.rag_pipeline.generate_embedding", new=AsyncMock(return_value=[0.1] * 768)), \
         patch("services.rag_pipeline.semantic_search", new=AsyncMock(return_value=[{
             "distance": 0.3,
             "content": "Test content",
             "act_id": 1
         }])):

        response = await generate_fast_response("Test question?", "user-123")

        assert response["query_id"] > 0
        assert response["response"]["response_type"] == "fast"
        assert "content" in response["response"]

@pytest.mark.asyncio
async def test_generate_fast_response_no_relevant_acts():
    """Test error when no relevant acts found"""
    with patch("services.rag_pipeline.semantic_search", new=AsyncMock(return_value=[])):

        with pytest.raises(NoRelevantActsError):
            await generate_fast_response("Test question?", "user-123")
```

---

## Environment Variables

### backend/.env

**IMPORTANT:** Backend is **deployment-agnostic** and connects to services via environment variables.

**Example for all-in-one deployment:**
```bash
# Supabase (local Docker, port 8444)
SUPABASE_URL=http://localhost:8444
SUPABASE_SERVICE_KEY=your-service-role-key-here
SUPABASE_JWT_SECRET=your-jwt-secret-here

# OLLAMA (local, port 11434)
OLLAMA_HOST=http://localhost:11434

# Redis (optional, for caching RAG context)
REDIS_URL=redis://localhost:6379/0

# Logging
LOG_LEVEL=INFO
DEBUG=true
```

**Example for distributed deployment:**
```bash
# Supabase (remote server, port 8444)
SUPABASE_URL=http://192.168.0.11:8444  # Replace with your server IP
SUPABASE_SERVICE_KEY=your-service-role-key-here
SUPABASE_JWT_SECRET=your-jwt-secret-here

# OLLAMA (remote server, port 11434)
OLLAMA_HOST=http://192.168.0.11:11434  # Replace with your server IP

# Redis (optional)
REDIS_URL=redis://localhost:6379/0
# Or remote: redis://192.168.0.11:6379/0

# Logging
LOG_LEVEL=INFO
DEBUG=true
```

**Example for cloud deployment:**
```bash
# Supabase Cloud
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key-here
SUPABASE_JWT_SECRET=your-jwt-secret-here

# OLLAMA (dedicated server)
OLLAMA_HOST=https://ollama.your-domain.com:11434

# Redis (cloud provider)
REDIS_URL=redis://your-redis-provider.com:6379/0

# Logging
LOG_LEVEL=INFO
DEBUG=false
```

**Verify connectivity before running backend:**
```bash
# Test services (replace URLs with your actual configuration)
curl ${SUPABASE_URL}/health
curl ${OLLAMA_HOST}/api/version

# Examples:
# curl http://localhost:8444/health
# curl http://192.168.0.11:8444/health
# curl https://your-project.supabase.co/health
```

### Load in code
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    supabase_url: str
    supabase_service_key: str
    supabase_jwt_secret: str
    ollama_host: str
    redis_url: str | None = None
    log_level: str = "INFO"

    class Config:
        env_file = ".env"

settings = Settings()
```

---

## Logging

### Configure Logging
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("app.log")
    ]
)

logger = logging.getLogger(__name__)

# Usage
logger.info("Query processed successfully")
logger.error("OLLAMA timeout", exc_info=True)
```

---

## Code Quality Tools

### 1. Ruff (Linter + Formatter)
```bash
pip install ruff

# Lint
ruff check backend/

# Format
ruff format backend/
```

### 2. Mypy (Type Checker)
```bash
pip install mypy

mypy backend/ --strict
```

### 3. Pre-commit Hooks
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.0
    hooks:
      - id: ruff
      - id: ruff-format
```

---

## Performance Checklist

- [ ] All I/O operations use async/await
- [ ] Database queries use connection pooling
- [ ] OLLAMA requests have appropriate timeouts
- [ ] Expensive operations cached (Redis)
- [ ] Batch processing for embeddings (not one-by-one)
- [ ] Proper indexing on database (pgvector IVFFlat)
- [ ] Rate limiting implemented (10 req/min per user)
- [ ] Logging configured (don't log sensitive data)
